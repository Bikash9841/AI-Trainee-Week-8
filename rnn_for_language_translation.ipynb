{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bikasherl/miniconda3/envs/bajra/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# tokenizer=AutoTokenizer.from_pretrained('gpt2')\n",
    "# tokenizer\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"CohleM/english-to-nepali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ne'],\n",
       "    num_rows: 177334\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7b8ea60f72f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader=DataLoader(ds['train'],batch_size=1)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for the tokenized data\n",
    "t_dict={'en':[],\n",
    "        'ne':[]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary of tokens for both english and nepali language\n",
    "\n",
    "# extract every sentence from the dataloader and tokenize it one by one.. not a good method. need to add padding manually by\n",
    "# deriving the maximum length of tokens for each batch and after that add padding to make the same length.\n",
    "# padding is needed beacuse we are making the batch. otherwise it is not needed in RNN.\n",
    "\n",
    "# for data in train_dataloader:\n",
    "\n",
    "#     for sentence in data['en']:\n",
    "#         t_dict['en'].append(tokenizer.encode(sentence))\n",
    "    \n",
    "#     for sentence in data['ne']:\n",
    "#         t_dict['ne'].append(tokenizer.encode(sentence))\n",
    "    \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 doesnt have special tokens to adding a padding token and start of sentence token\n",
    "# tokenizer.add_special_tokens({'pad_token':'<pad>'})\n",
    "# tokenizer.add_special_tokens({'bos_token':'<sos>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # finding maximum sentence length for each batch for english langauge\n",
    "# sen_length=[]\n",
    "# for sen in t_dict['en']:\n",
    "#     sen_length.append(len(sen))\n",
    "\n",
    "# # add zero padding to all token if not of same length(max length)\n",
    "# for sen in t_dict['en']:\n",
    "#     for i in range(len(sen),max(sen_length)):\n",
    "#         sen.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding maximum sentence length\n",
    "# max(sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Encoder Decoder Architecture for Language Translation Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(1001,128)\n",
    "        self.rnn=nn.RNN(128,hidden_size=32,batch_first=True)\n",
    "\n",
    "    def forward(self,x,hidden=None):\n",
    "        x=self.embedding(x)\n",
    "        x,hidden=self.rnn(x,hidden)\n",
    "        return x,hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(1001, 128)\n",
       "  (rnn): RNN(128, 32, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn=Encoder()\n",
    "model_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_out,enc_hid=model_rnn(torch.tensor(t_dict['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_out.shape,enc_hid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(1001,128)\n",
    "        self.rnn=nn.RNN(128,hidden_size=32,batch_first=True)\n",
    "        self.linear=nn.Linear(32,1001)\n",
    "        \n",
    "    \n",
    "    def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
    "\n",
    "        batch_size=encoder_outputs.shape[0]\n",
    "        decoder_input=torch.empty(batch_size,1,dtype=torch.long,device='cuda').fill_(torch.tensor(1000))#(torch.tensor(tokenizer.encode(tokenizer.bos_token)[0]))\n",
    "        decoder_hidden=encoder_hidden\n",
    "        decoder_outputs=[]\n",
    "\n",
    "        for i in range(target_tensor.shape[1]):\n",
    "            decoder_out,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
    "            decoder_outputs.append(decoder_out)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input=target_tensor[:,i].unsqueeze(1)\n",
    "            else:\n",
    "                # return \"Not Inference Time! \"\n",
    "                pass\n",
    "        \n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        # decoder_outputs = nn.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden\n",
    "        \n",
    "    def forward_step(self,input,hidden):\n",
    "        out=self.embedding(input)\n",
    "        out,hidden=self.rnn(out,hidden)\n",
    "        out=self.linear(out)\n",
    "        return out,hidden\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(1001, 128)\n",
       "  (rnn): RNN(128, 32, batch_first=True)\n",
       "  (linear): Linear(in_features=32, out_features=1001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_rnn=Decoder()\n",
    "dec_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec_out,dec_hidden=dec_rnn(encoder_outputs=enc_out,encoder_hidden=enc_hid,target_tensor=torch.tensor(t_dict['ne']))\n",
    "# dec_out.shape,dec_hidden.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outty=dec_out.argmax(dim=-1)\n",
    "# outty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(outty[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" \".join(ds['train']['en'][:5000]) + \" \" + \" \".join(ds['train']['ne'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toke():\n",
    "    def __init__(self,text,final_vocab_size):\n",
    "        self.final_vocab_size=final_vocab_size\n",
    "        self.tokens = text.encode(\"utf-8\") # raw bytes\n",
    "        self.tokens = list(map(int, self.tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "        self.ids = list(self.tokens) # copy so we don't destroy the original list\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab={idx: bytes([idx]) for idx in range(256)}\n",
    "    \n",
    "    def get_stats(self,ids):\n",
    "        self.counts={}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            self.counts[pair] = self.counts.get(pair, 0) + 1\n",
    "        return self.counts\n",
    "    \n",
    "    def merge(self,ids, pair, idx):\n",
    "        i = 0\n",
    "        self.newids=[]\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                self.newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                self.newids.append(ids[i])\n",
    "                i += 1\n",
    "        return self.newids\n",
    "    \n",
    "    # make the vocab reamining\n",
    "    def create_final_vocab(self):\n",
    "\n",
    "        # create the merge dictionary using BPE algo\n",
    "        for i in range(self.final_vocab_size-256):\n",
    "            stats = self.get_stats(self.ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            self.ids = self.merge(self.ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "\n",
    "        # create final vocab\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "\n",
    "    def encode(self,text):\n",
    "        # given a string, return list of integers (the tokens)\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self.get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer object\n",
    "\n",
    "otoke=Toke(text=text,final_vocab_size=1000)\n",
    "otoke.vocab=torch.load('/home/bikasherl/Desktop/Week 8/vocab.pt')\n",
    "otoke.vocab[1000]=b'<sos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final vocab after performing BPE\n",
    "\n",
    "# otoke.create_final_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Language Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer=torch.optim.Adam(model_rnn.parameters(),lr=0.01)\n",
    "decoder_optimizer=torch.optim.Adam(dec_rnn.parameters(),lr=0.01)\n",
    "\n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "for epoch in range(10):\n",
    "  total_loss = 0\n",
    "  batch=0\n",
    "  for data in train_dataloader:\n",
    "\n",
    "      batch+=1\n",
    "      # t_dict['en']=tokenizer(data['en'],padding=True)['input_ids']\n",
    "      # t_dict['ne']=tokenizer(data['ne'],padding=True)['input_ids']\n",
    "\n",
    "      t_dict['en']=torch.tensor(otoke.encode(data['en'][0])).unsqueeze(0)\n",
    "      t_dict['ne']=torch.tensor(otoke.encode(data['ne'][0])).unsqueeze(0)\n",
    "\n",
    "      encoder_optimizer.zero_grad()\n",
    "      decoder_optimizer.zero_grad()\n",
    "\n",
    "      enc_out,enc_hid=model_rnn((t_dict['en']).to(device))\n",
    "      dec_out,dec_hidden=dec_rnn(encoder_outputs=enc_out,encoder_hidden=enc_hid,target_tensor=(t_dict['ne']).to(device))\n",
    "\n",
    "      loss = criterion(\n",
    "          dec_out.view(-1, dec_out.size(-1)),\n",
    "          torch.tensor(t_dict['ne']).to(device).view(-1)\n",
    "      )\n",
    "      loss.backward()\n",
    "\n",
    "      encoder_optimizer.step()\n",
    "      decoder_optimizer.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      if batch>500:\n",
    "        break\n",
    "  train_loss.append(total_loss / 500)\n",
    "  print(f\"Epoch: {epoch}  Loss: {total_loss/500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 294, 1001])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otoke.decode(dec_out.argmax(dim=-1).squeeze(0).tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bajra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
